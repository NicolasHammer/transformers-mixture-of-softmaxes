{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07dfc39e2c86bf15d26cdaf7af3b6a503035b5583dc7ceba661ecec139caebdd8",
   "display_name": "Python 3.8.3 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Breaking the Transformer Bottleneck"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this notebook, we will examine how the Mixture of Softmaxes model proposed in [Yang et al. (2018)](https://arxiv.org/pdf/1711.03953.pdf) affects the performance of an encoder-only Transformer as outlined in [Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf).  Our workflow closely follows the [Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on the PyTorch website.  We begin by importing relevant packages that will be used throughout this notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# Importing packages used throughout\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "## Importing custom files\n",
    "from model import transformer_model\n",
    "from data_import import batching\n",
    "\n",
    "## Establishing devices\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f326a4c8870>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Establishing randomness\n",
    "torch.manual_seed(26)\n"
   ]
  },
  {
   "source": [
    "# Transformer Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Below are the hyperparameters chosen for the models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish hyperparameters\n",
    "\n",
    "emsize = 300 # embedding dimension\n",
    "nhid = 300 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "num_softmaxes = 10 # number of softmaxes\n",
    "epochs = 3 # number of epochs\n",
    "lr = 7.0 # learning rate\n",
    "gradient_clip = 0.25 # what to clip the gradients by"
   ]
  },
  {
   "source": [
    "## Establish Training/Evaluating Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As mentioned before, this is a encoder-only Transformer, as defined in [transformer_model.py](/model/transformer_model.py).  We will be using negative-log-likelihood loss as well as stochastic gradient descent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(model: transformer_model.TransformerModel, train_data: torch.Tensor, optimizer: torch.optim.SGD, learning_rate: float) -> None:\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(chunk_length).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, chunk_length)):\n",
    "        data, targets = batching.get_batch(train_data, i, chunk_length)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != chunk_length:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // chunk_length, learning_rate,\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model: transformer_model.TransformerModel, data_source: torch.Tensor) -> float:\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = eval_model.generate_square_subsequent_mask(chunk_length).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, chunk_length):\n",
    "            data, targets = batching.get_batch(data_source, i, chunk_length)\n",
    "            if data.size(0) != chunk_length:\n",
    "                src_mask = eval_model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "source": [
    "# WikiText-2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first dataset we will model the language of is [WikiText-2](https://paperswithcode.com/dataset/wikitext-2).  The data is downloaded into the [.data](/.data) folder."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "from torchtext.datasets import WikiText2\n",
    "from data_import import wikitext"
   ]
  },
  {
   "source": [
    "## Preparing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing hyperparameters\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "chunk_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up vocabulary\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "ntokens = len(vocab.stoi) # the size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = wikitext.data_process(train_iter, vocab, tokenizer)\n",
    "val_data = wikitext.data_process(val_iter, vocab, tokenizer)\n",
    "test_data = wikitext.data_process(test_iter, vocab, tokenizer)\n",
    "\n",
    "## Batch data\n",
    "train_data = batching.batchify(train_data, batch_size, device)\n",
    "val_data = batching.batchify(val_data, eval_batch_size, device)\n",
    "test_data = batching.batchify(test_data, eval_batch_size, device)"
   ]
  },
  {
   "source": [
    "## Train the Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 7.00 | ms/batch 16.23 | loss  8.15 | ppl  3465.77\n",
      "| epoch   1 |   400/ 2928 batches | lr 7.00 | ms/batch 15.90 | loss  6.90 | ppl   987.74\n",
      "| epoch   1 |   600/ 2928 batches | lr 7.00 | ms/batch 15.92 | loss  6.47 | ppl   648.32\n",
      "| epoch   1 |   800/ 2928 batches | lr 7.00 | ms/batch 15.96 | loss  6.31 | ppl   551.89\n",
      "| epoch   1 |  1000/ 2928 batches | lr 7.00 | ms/batch 15.95 | loss  6.17 | ppl   479.10\n",
      "| epoch   1 |  1200/ 2928 batches | lr 7.00 | ms/batch 15.98 | loss  6.13 | ppl   460.62\n",
      "| epoch   1 |  1400/ 2928 batches | lr 7.00 | ms/batch 16.02 | loss  6.07 | ppl   430.85\n",
      "| epoch   1 |  1600/ 2928 batches | lr 7.00 | ms/batch 16.04 | loss  6.05 | ppl   424.57\n",
      "| epoch   1 |  1800/ 2928 batches | lr 7.00 | ms/batch 16.03 | loss  5.95 | ppl   385.41\n",
      "| epoch   1 |  2000/ 2928 batches | lr 7.00 | ms/batch 16.03 | loss  5.94 | ppl   378.52\n",
      "| epoch   1 |  2200/ 2928 batches | lr 7.00 | ms/batch 16.04 | loss  5.81 | ppl   331.97\n",
      "| epoch   1 |  2400/ 2928 batches | lr 7.00 | ms/batch 16.05 | loss  5.88 | ppl   357.56\n",
      "| epoch   1 |  2600/ 2928 batches | lr 7.00 | ms/batch 16.04 | loss  5.86 | ppl   351.28\n",
      "| epoch   1 |  2800/ 2928 batches | lr 7.00 | ms/batch 16.01 | loss  5.77 | ppl   321.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 48.90s | valid loss  5.70 | valid ppl   299.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 7.00 | ms/batch 16.16 | loss  5.77 | ppl   321.63\n",
      "| epoch   2 |   400/ 2928 batches | lr 7.00 | ms/batch 16.15 | loss  5.77 | ppl   320.22\n",
      "| epoch   2 |   600/ 2928 batches | lr 7.00 | ms/batch 16.15 | loss  5.57 | ppl   261.55\n",
      "| epoch   2 |   800/ 2928 batches | lr 7.00 | ms/batch 16.07 | loss  5.60 | ppl   271.51\n",
      "| epoch   2 |  1000/ 2928 batches | lr 7.00 | ms/batch 16.14 | loss  5.54 | ppl   255.04\n",
      "| epoch   2 |  1200/ 2928 batches | lr 7.00 | ms/batch 16.17 | loss  5.57 | ppl   263.02\n",
      "| epoch   2 |  1400/ 2928 batches | lr 7.00 | ms/batch 16.15 | loss  5.58 | ppl   264.65\n",
      "| epoch   2 |  1600/ 2928 batches | lr 7.00 | ms/batch 16.17 | loss  5.60 | ppl   270.62\n",
      "| epoch   2 |  1800/ 2928 batches | lr 7.00 | ms/batch 16.16 | loss  5.53 | ppl   251.42\n",
      "| epoch   2 |  2000/ 2928 batches | lr 7.00 | ms/batch 16.07 | loss  5.53 | ppl   253.35\n",
      "| epoch   2 |  2200/ 2928 batches | lr 7.00 | ms/batch 16.12 | loss  5.41 | ppl   223.84\n",
      "| epoch   2 |  2400/ 2928 batches | lr 7.00 | ms/batch 16.13 | loss  5.51 | ppl   247.23\n",
      "| epoch   2 |  2600/ 2928 batches | lr 7.00 | ms/batch 16.10 | loss  5.51 | ppl   246.17\n",
      "| epoch   2 |  2800/ 2928 batches | lr 7.00 | ms/batch 16.09 | loss  5.42 | ppl   226.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 49.23s | valid loss  5.50 | valid ppl   244.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 7.00 | ms/batch 16.17 | loss  5.46 | ppl   235.44\n",
      "| epoch   3 |   400/ 2928 batches | lr 7.00 | ms/batch 16.09 | loss  5.50 | ppl   243.53\n",
      "| epoch   3 |   600/ 2928 batches | lr 7.00 | ms/batch 16.10 | loss  5.28 | ppl   196.31\n",
      "| epoch   3 |   800/ 2928 batches | lr 7.00 | ms/batch 16.10 | loss  5.34 | ppl   207.69\n",
      "| epoch   3 |  1000/ 2928 batches | lr 7.00 | ms/batch 16.13 | loss  5.28 | ppl   197.27\n",
      "| epoch   3 |  1200/ 2928 batches | lr 7.00 | ms/batch 16.09 | loss  5.33 | ppl   206.93\n",
      "| epoch   3 |  1400/ 2928 batches | lr 7.00 | ms/batch 16.10 | loss  5.35 | ppl   210.36\n",
      "| epoch   3 |  1600/ 2928 batches | lr 7.00 | ms/batch 16.10 | loss  5.38 | ppl   215.99\n",
      "| epoch   3 |  1800/ 2928 batches | lr 7.00 | ms/batch 16.09 | loss  5.31 | ppl   202.24\n",
      "| epoch   3 |  2000/ 2928 batches | lr 7.00 | ms/batch 16.11 | loss  5.32 | ppl   204.34\n",
      "| epoch   3 |  2200/ 2928 batches | lr 7.00 | ms/batch 16.14 | loss  5.18 | ppl   178.32\n",
      "| epoch   3 |  2400/ 2928 batches | lr 7.00 | ms/batch 16.18 | loss  5.29 | ppl   198.04\n",
      "| epoch   3 |  2600/ 2928 batches | lr 7.00 | ms/batch 16.19 | loss  5.30 | ppl   200.79\n",
      "| epoch   3 |  2800/ 2928 batches | lr 7.00 | ms/batch 16.17 | loss  5.22 | ppl   184.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 49.22s | valid loss  5.41 | valid ppl   223.65\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train Stock Model\n",
    "wikitext_stock_model = transformer_model.TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout, 1).to(device)\n",
    "optimizer = torch.optim.SGD(wikitext_stock_model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_wikitext_stock_model = None\n",
    "learning_rate = lr\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(wikitext_stock_model, train_data, optimizer, learning_rate)\n",
    "    val_loss = evaluate(wikitext_stock_model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                    val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_wikitext_stock_model = wikitext_stock_model\n",
    "    else:\n",
    "        learning_rate = learning_rate / 1.75\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 7.00 | ms/batch 115.46 | loss  9.67 | ppl 15771.67\n",
      "| epoch   1 |   400/ 2928 batches | lr 7.00 | ms/batch 114.91 | loss  7.65 | ppl  2096.44\n",
      "| epoch   1 |   600/ 2928 batches | lr 7.00 | ms/batch 115.04 | loss  6.96 | ppl  1051.03\n",
      "| epoch   1 |   800/ 2928 batches | lr 7.00 | ms/batch 115.10 | loss  6.48 | ppl   652.72\n",
      "| epoch   1 |  1000/ 2928 batches | lr 7.00 | ms/batch 115.35 | loss  6.26 | ppl   521.74\n",
      "| epoch   1 |  1200/ 2928 batches | lr 7.00 | ms/batch 115.37 | loss  6.19 | ppl   486.39\n",
      "| epoch   1 |  1400/ 2928 batches | lr 7.00 | ms/batch 115.49 | loss  6.10 | ppl   447.44\n",
      "| epoch   1 |  1600/ 2928 batches | lr 7.00 | ms/batch 115.46 | loss  6.09 | ppl   439.86\n",
      "| epoch   1 |  1800/ 2928 batches | lr 7.00 | ms/batch 115.43 | loss  5.99 | ppl   400.12\n",
      "| epoch   1 |  2000/ 2928 batches | lr 7.00 | ms/batch 115.50 | loss  5.97 | ppl   392.90\n",
      "| epoch   1 |  2200/ 2928 batches | lr 7.00 | ms/batch 115.47 | loss  5.85 | ppl   346.28\n",
      "| epoch   1 |  2400/ 2928 batches | lr 7.00 | ms/batch 115.55 | loss  5.91 | ppl   370.26\n",
      "| epoch   1 |  2600/ 2928 batches | lr 7.00 | ms/batch 115.53 | loss  5.89 | ppl   363.00\n",
      "| epoch   1 |  2800/ 2928 batches | lr 7.00 | ms/batch 115.41 | loss  5.82 | ppl   335.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 350.14s | valid loss  5.74 | valid ppl   311.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 7.00 | ms/batch 116.13 | loss  5.83 | ppl   341.01\n",
      "| epoch   2 |   400/ 2928 batches | lr 7.00 | ms/batch 115.54 | loss  5.85 | ppl   345.69\n",
      "| epoch   2 |   600/ 2928 batches | lr 7.00 | ms/batch 115.61 | loss  5.67 | ppl   290.29\n",
      "| epoch   2 |   800/ 2928 batches | lr 7.00 | ms/batch 115.59 | loss  5.69 | ppl   294.97\n",
      "| epoch   2 |  1000/ 2928 batches | lr 7.00 | ms/batch 115.49 | loss  5.62 | ppl   274.68\n",
      "| epoch   2 |  1200/ 2928 batches | lr 7.00 | ms/batch 115.52 | loss  5.65 | ppl   283.69\n",
      "| epoch   2 |  1400/ 2928 batches | lr 7.00 | ms/batch 115.50 | loss  5.64 | ppl   281.22\n",
      "| epoch   2 |  1600/ 2928 batches | lr 7.00 | ms/batch 115.57 | loss  5.66 | ppl   287.91\n",
      "| epoch   2 |  1800/ 2928 batches | lr 7.00 | ms/batch 115.62 | loss  5.60 | ppl   269.24\n",
      "| epoch   2 |  2000/ 2928 batches | lr 7.00 | ms/batch 115.59 | loss  5.60 | ppl   271.51\n",
      "| epoch   2 |  2200/ 2928 batches | lr 7.00 | ms/batch 115.57 | loss  5.48 | ppl   240.12\n",
      "| epoch   2 |  2400/ 2928 batches | lr 7.00 | ms/batch 115.58 | loss  5.57 | ppl   262.73\n",
      "| epoch   2 |  2600/ 2928 batches | lr 7.00 | ms/batch 115.58 | loss  5.57 | ppl   262.99\n",
      "| epoch   2 |  2800/ 2928 batches | lr 7.00 | ms/batch 115.55 | loss  5.49 | ppl   243.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 350.85s | valid loss  5.51 | valid ppl   246.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 7.00 | ms/batch 116.35 | loss  5.54 | ppl   253.46\n",
      "| epoch   3 |   400/ 2928 batches | lr 7.00 | ms/batch 115.81 | loss  5.57 | ppl   263.36\n",
      "| epoch   3 |   600/ 2928 batches | lr 7.00 | ms/batch 115.64 | loss  5.38 | ppl   217.94\n",
      "| epoch   3 |   800/ 2928 batches | lr 7.00 | ms/batch 115.53 | loss  5.43 | ppl   228.56\n",
      "| epoch   3 |  1000/ 2928 batches | lr 7.00 | ms/batch 115.55 | loss  5.38 | ppl   216.26\n",
      "| epoch   3 |  1200/ 2928 batches | lr 7.00 | ms/batch 115.56 | loss  5.42 | ppl   226.01\n",
      "| epoch   3 |  1400/ 2928 batches | lr 7.00 | ms/batch 115.58 | loss  5.42 | ppl   226.38\n",
      "| epoch   3 |  1600/ 2928 batches | lr 7.00 | ms/batch 115.63 | loss  5.45 | ppl   233.76\n",
      "| epoch   3 |  1800/ 2928 batches | lr 7.00 | ms/batch 115.72 | loss  5.39 | ppl   218.95\n",
      "| epoch   3 |  2000/ 2928 batches | lr 7.00 | ms/batch 115.94 | loss  5.40 | ppl   221.07\n",
      "| epoch   3 |  2200/ 2928 batches | lr 7.00 | ms/batch 115.73 | loss  5.27 | ppl   193.76\n",
      "| epoch   3 |  2400/ 2928 batches | lr 7.00 | ms/batch 115.70 | loss  5.37 | ppl   214.80\n",
      "| epoch   3 |  2600/ 2928 batches | lr 7.00 | ms/batch 115.70 | loss  5.38 | ppl   216.86\n",
      "| epoch   3 |  2800/ 2928 batches | lr 7.00 | ms/batch 115.75 | loss  5.30 | ppl   200.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 351.28s | valid loss  5.42 | valid ppl   225.05\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the MoS Model\n",
    "wikitext_MoS_model = transformer_model.TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout, num_softmaxes).to(device)\n",
    "optimizer = torch.optim.SGD(wikitext_MoS_model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_wikitext_MoS_model = None\n",
    "learning_rate = lr\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(wikitext_MoS_model, train_data, optimizer, learning_rate)\n",
    "    val_loss = evaluate(wikitext_MoS_model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                    val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_wikitext_MoS_model = wikitext_MoS_model\n",
    "    else:\n",
    "        learning_rate = learning_rate / 1.75\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate"
   ]
  },
  {
   "source": [
    "## Evaluate the Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  5.33 | test ppl   205.98\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Stock Model\n",
    "test_loss = evaluate(best_wikitext_stock_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  5.33 | test ppl   205.53\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MoS Model\n",
    "test_loss = evaluate(best_wikitext_MoS_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "source": [
    "# Penn Treebank"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The second dataset we will model the language of is the [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "from torchtext.datasets import PennTreebank\n",
    "from data_import import pentree_bank"
   ]
  },
  {
   "source": [
    "## Preparing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing hyperparameters\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "chunk_length = 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up vocabulary\n",
    "train_iter, valid_iter, test_iter = PennTreebank()\n",
    "corpus = pentree_bank.Corpus(train_iter, valid_iter, test_iter)\n",
    "\n",
    "ntokens = len(corpus.dictionary) # the size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch data\n",
    "train_data = batching.batchify(corpus.train, batch_size, device)\n",
    "val_data = batching.batchify(corpus.valid, eval_batch_size, device)\n",
    "test_data = batching.batchify(corpus.test, eval_batch_size, device)"
   ]
  },
  {
   "source": [
    "## Train the Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| epoch   1 |   200/ 1388 batches | lr 7.00 | ms/batch  9.46 | loss  6.98 | ppl  1078.86\n",
      "| epoch   1 |   400/ 1388 batches | lr 7.00 | ms/batch  9.37 | loss  5.93 | ppl   376.72\n",
      "| epoch   1 |   600/ 1388 batches | lr 7.00 | ms/batch  9.49 | loss  5.66 | ppl   288.54\n",
      "| epoch   1 |   800/ 1388 batches | lr 7.00 | ms/batch  9.48 | loss  5.50 | ppl   245.16\n",
      "| epoch   1 |  1000/ 1388 batches | lr 7.00 | ms/batch  9.43 | loss  5.39 | ppl   218.21\n",
      "| epoch   1 |  1200/ 1388 batches | lr 7.00 | ms/batch  9.39 | loss  5.28 | ppl   197.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 13.61s | valid loss  5.28 | valid ppl   195.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1388 batches | lr 7.00 | ms/batch  9.49 | loss  5.19 | ppl   179.26\n",
      "| epoch   2 |   400/ 1388 batches | lr 7.00 | ms/batch  9.43 | loss  5.12 | ppl   167.63\n",
      "| epoch   2 |   600/ 1388 batches | lr 7.00 | ms/batch  9.46 | loss  5.06 | ppl   157.69\n",
      "| epoch   2 |   800/ 1388 batches | lr 7.00 | ms/batch  9.53 | loss  5.01 | ppl   150.05\n",
      "| epoch   2 |  1000/ 1388 batches | lr 7.00 | ms/batch  9.49 | loss  4.99 | ppl   146.67\n",
      "| epoch   2 |  1200/ 1388 batches | lr 7.00 | ms/batch  9.48 | loss  4.92 | ppl   137.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.68s | valid loss  4.99 | valid ppl   147.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1388 batches | lr 7.00 | ms/batch  9.51 | loss  4.90 | ppl   134.64\n",
      "| epoch   3 |   400/ 1388 batches | lr 7.00 | ms/batch  9.40 | loss  4.87 | ppl   130.68\n",
      "| epoch   3 |   600/ 1388 batches | lr 7.00 | ms/batch  9.38 | loss  4.83 | ppl   124.81\n",
      "| epoch   3 |   800/ 1388 batches | lr 7.00 | ms/batch  9.54 | loss  4.80 | ppl   120.93\n",
      "| epoch   3 |  1000/ 1388 batches | lr 7.00 | ms/batch  9.68 | loss  4.80 | ppl   121.51\n",
      "| epoch   3 |  1200/ 1388 batches | lr 7.00 | ms/batch  9.53 | loss  4.74 | ppl   114.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.73s | valid loss  4.92 | valid ppl   136.34\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train Stock Model\n",
    "penntree_stock_model = transformer_model.TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout, 1).to(device)\n",
    "optimizer = torch.optim.SGD(penntree_stock_model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_penntree_stock_model = None\n",
    "learning_rate = lr\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(penntree_stock_model, train_data, optimizer, learning_rate)\n",
    "    val_loss = evaluate(penntree_stock_model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                    val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_penntree_stock_model = penntree_stock_model\n",
    "    else:\n",
    "        learning_rate = learning_rate / 1.75\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| epoch   1 |   200/ 1388 batches | lr 7.00 | ms/batch 43.37 | loss  9.02 | ppl  8254.13\n",
      "| epoch   1 |   400/ 1388 batches | lr 7.00 | ms/batch 43.36 | loss  6.73 | ppl   833.80\n",
      "| epoch   1 |   600/ 1388 batches | lr 7.00 | ms/batch 43.11 | loss  6.14 | ppl   462.39\n",
      "| epoch   1 |   800/ 1388 batches | lr 7.00 | ms/batch 43.12 | loss  5.74 | ppl   311.25\n",
      "| epoch   1 |  1000/ 1388 batches | lr 7.00 | ms/batch 43.10 | loss  5.54 | ppl   254.95\n",
      "| epoch   1 |  1200/ 1388 batches | lr 7.00 | ms/batch 43.07 | loss  5.42 | ppl   224.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 61.62s | valid loss  5.34 | valid ppl   207.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1388 batches | lr 7.00 | ms/batch 43.35 | loss  5.32 | ppl   203.38\n",
      "| epoch   2 |   400/ 1388 batches | lr 7.00 | ms/batch 43.22 | loss  5.26 | ppl   192.88\n",
      "| epoch   2 |   600/ 1388 batches | lr 7.00 | ms/batch 43.22 | loss  5.20 | ppl   182.15\n",
      "| epoch   2 |   800/ 1388 batches | lr 7.00 | ms/batch 43.24 | loss  5.13 | ppl   169.36\n",
      "| epoch   2 |  1000/ 1388 batches | lr 7.00 | ms/batch 43.18 | loss  5.09 | ppl   162.62\n",
      "| epoch   2 |  1200/ 1388 batches | lr 7.00 | ms/batch 43.32 | loss  5.03 | ppl   152.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 61.78s | valid loss  5.07 | valid ppl   159.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1388 batches | lr 7.00 | ms/batch 43.39 | loss  5.01 | ppl   150.64\n",
      "| epoch   3 |   400/ 1388 batches | lr 7.00 | ms/batch 43.23 | loss  5.00 | ppl   148.78\n",
      "| epoch   3 |   600/ 1388 batches | lr 7.00 | ms/batch 43.23 | loss  4.97 | ppl   143.43\n",
      "| epoch   3 |   800/ 1388 batches | lr 7.00 | ms/batch 43.31 | loss  4.92 | ppl   137.59\n",
      "| epoch   3 |  1000/ 1388 batches | lr 7.00 | ms/batch 43.10 | loss  4.92 | ppl   136.33\n",
      "| epoch   3 |  1200/ 1388 batches | lr 7.00 | ms/batch 43.16 | loss  4.85 | ppl   128.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 61.69s | valid loss  4.97 | valid ppl   144.53\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the MoS Model\n",
    "penntree_MoS_model = transformer_model.TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout, num_softmaxes).to(device)\n",
    "optimizer = torch.optim.SGD(penntree_MoS_model.parameters(), lr=lr)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_penntree_MoS_model = None\n",
    "learning_rate = lr\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(penntree_MoS_model, train_data, optimizer, learning_rate)\n",
    "    val_loss = evaluate(penntree_MoS_model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                    val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_penntree_MoS_model = penntree_MoS_model\n",
    "    else:\n",
    "        learning_rate = learning_rate / 1.75\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate"
   ]
  },
  {
   "source": [
    "## Evaluate the Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  4.88 | test ppl   131.62\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Stock Model\n",
    "test_loss = evaluate(best_penntree_stock_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=========================================================================================\n| End of training | test loss  4.93 | test ppl   138.42\n=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MoS Model\n",
    "test_loss = evaluate(best_penntree_MoS_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "source": [
    "With only three epochs, each model (stock and MoS) performs about the same.  If we train the models for many more epochs, though, say 50, it becomes apparent that the MoS achieves a significantly lower perplexity, thus proving its usefulness in the encoder-only Transformer architecture."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}